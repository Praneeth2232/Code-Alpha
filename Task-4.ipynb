{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f1141-b4fd-4bf8-b2f2-b500c1ed90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 1834.0ms\n",
      "Speed: 10.7ms preprocess, 1834.0ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1914.0ms\n",
      "Speed: 9.9ms preprocess, 1914.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 2011.7ms\n",
      "Speed: 6.0ms preprocess, 2011.7ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1650.9ms\n",
      "Speed: 6.0ms preprocess, 1650.9ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1444.2ms\n",
      "Speed: 12.0ms preprocess, 1444.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1836.7ms\n",
      "Speed: 12.6ms preprocess, 1836.7ms inference, 9.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1320.2ms\n",
      "Speed: 6.3ms preprocess, 1320.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1000.1ms\n",
      "Speed: 4.0ms preprocess, 1000.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1016.1ms\n",
      "Speed: 3.4ms preprocess, 1016.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1018.1ms\n",
      "Speed: 4.0ms preprocess, 1018.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 970.7ms\n",
      "Speed: 3.3ms preprocess, 970.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 994.5ms\n",
      "Speed: 4.0ms preprocess, 994.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1009.1ms\n",
      "Speed: 4.2ms preprocess, 1009.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1043.9ms\n",
      "Speed: 3.7ms preprocess, 1043.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1775.4ms\n",
      "Speed: 15.2ms preprocess, 1775.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1027.5ms\n",
      "Speed: 3.4ms preprocess, 1027.5ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 999.2ms\n",
      "Speed: 3.0ms preprocess, 999.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1589.8ms\n",
      "Speed: 3.4ms preprocess, 1589.8ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1795.7ms\n",
      "Speed: 8.0ms preprocess, 1795.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 1705.1ms\n",
      "Speed: 4.9ms preprocess, 1705.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import argparse\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "import numpy as np\n",
    "\n",
    "ZONE_POLYGON = np.array([\n",
    "    [0, 0],\n",
    "    [0.5, 0],\n",
    "    [0.5, 1],\n",
    "    [0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"YOLOv8 live\")\n",
    "    args, unknown = parser.parse_known_args()  # Handle unknown args in Jupyter\n",
    "    return args\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    model = YOLO(\"yolov8l.pt\")\n",
    "    \n",
    "    bounding_box_annotator = sv.BoxAnnotator(thickness=2)\n",
    "    label_annotator = sv.LabelAnnotator(text_thickness=2, text_scale=1)\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        return\n",
    "    \n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    zone_polygon = (ZONE_POLYGON * np.array([frame_width, frame_height])).astype(int)\n",
    "    zone = sv.PolygonZone(polygon=zone_polygon)\n",
    "    zone_annotator = sv.PolygonZoneAnnotator(\n",
    "        zone=zone,\n",
    "        color=sv.Color.RED,\n",
    "        thickness=2,\n",
    "        text_thickness=4,\n",
    "        text_scale=2\n",
    "    )\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        result = model(frame, agnostic_nms=True)[0]\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        confidences = result.boxes.conf.cpu().numpy()\n",
    "        class_ids = result.boxes.cls.cpu().numpy().astype(int)  # Convert to integers\n",
    "    \n",
    "        detections = sv.Detections(\n",
    "            xyxy=boxes,\n",
    "            confidence=confidences,\n",
    "            class_id=class_ids\n",
    "        )\n",
    "        labels = [\n",
    "            f\"{model.names[class_id]} {confidence:0.2f}\"\n",
    "            for confidence, class_id in zip(confidences, class_ids)\n",
    "        ]\n",
    "        frame = bounding_box_annotator.annotate(scene=frame, detections=detections)\n",
    "        frame = label_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    \n",
    "        zone.trigger(detections=detections)\n",
    "        frame = zone_annotator.annotate(scene=frame)\n",
    "    \n",
    "        try:\n",
    "            cv2.imshow(\"yolov8\", frame)\n",
    "        except cv2.error as e:\n",
    "            print(f\"Error displaying frame: {e}\")\n",
    "            cv2.imwrite(\"output_frame.jpg\", frame)\n",
    "            break\n",
    "    \n",
    "        if cv2.waitKey(30) == 27:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb3c39-9f6c-42ac-a2c2-fc91f964725d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
